{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pathlib\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import openai\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.docstore import Wikipedia\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.agents import AgentType\n",
    "from langchain.agents.react.base import DocstoreExplorer\n",
    "from langchain.agents import load_tools\n",
    "from langchain.tools import StructuredTool\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "\n",
    "from GithubConnector import GithubConnector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# openai.api_key = os.getenv(\"AZURE_OPENAI_KEY\")\n",
    "# openai.api_base = os.getenv(\"AZURE_OPENAI_ENDPOINT\") # your endpoint should look like the following https://YOUR_RESOURCE_NAME.openai.azure.com/\n",
    "# openai.api_type = 'azure'\n",
    "# openai.api_version = '2023-05-15' # this may change in the future\n",
    "from langchain.llms import AzureOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(dotenv_path=pathlib.Path('test.env')) # \".env\"\n",
    "\n",
    "llm = OpenAI(temperature=0, model_name=\"gpt-3.5-turbo-16k\") # # gpt-3.5-turbo\n",
    "#os.environ['OPENAI_API_KEY'] = \n",
    "# llm = AzureOpenAI(\n",
    "#     deployment_name=\"gpt-35-turbo-16k\",\n",
    "#     model_name=\"gpt-35-turbo-16k\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frontend_loader = DirectoryLoader(\"./demo_repos/frontend_sxsw_hackathon/\", glob=\"**/*.js\", recursive=True, show_progress=True, use_multithreading=False, loader_cls=TextLoader)\n",
    "\n",
    "frontend_docs = frontend_loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(frontend_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backend_loader = DirectoryLoader(\"./demo_repos/fastapi_demo_sxsw/\", glob=\"**/*.py\", recursive=True, show_progress=True, use_multithreading=False, loader_cls=TextLoader)\n",
    "\n",
    "backend_docs = backend_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(backend_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import glob\n",
    "\n",
    "# txt_files = glob.glob(\"./demo_repos/**/*.*\", recursive=True)\n",
    "# import pprint\n",
    "# pprint.pprint(txt_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"BAAI/bge-small-en\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': True}\n",
    "hf = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "frontend_texts = text_splitter.split_documents(frontend_docs)\n",
    "\n",
    "embeddings = hf\n",
    "frontend_docsearch = Chroma.from_documents(frontend_texts, embeddings, collection_name=\"frontend\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backend_texts = text_splitter.split_documents(backend_docs)\n",
    "\n",
    "embeddings = hf\n",
    "backend_docsearch = Chroma.from_documents(backend_texts, embeddings, collection_name=\"frontend\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = load_tools([\"llm-math\"], llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from langchain.tools import tool\n",
    "\n",
    "\n",
    "\n",
    "class Docstore:\n",
    "    def __init__(self, folder_path):\n",
    "        self.folder_path = folder_path\n",
    "\n",
    "    def list_files(self, *args, **kwargs):\n",
    "        \"\"\"List all files in the document store recursively.\"\"\"\n",
    "        all_files = []\n",
    "        for dirpath, dirnames, filenames in os.walk(self.folder_path):\n",
    "            for filename in filenames:\n",
    "                all_files.append(os.path.join(dirpath, filename))\n",
    "        return all_files\n",
    "\n",
    "    def read_file(self, filepath):\n",
    "        \"\"\"Read the content of a specific file.\"\"\"\n",
    "        filepath = filepath.replace(\"'\", \"\")\n",
    "        with open(filepath, 'rb') as file:\n",
    "            return file.read()\n",
    "        \n",
    "\n",
    "github_connector = GithubConnector(github_access_token=os.getenv(\"GITHUB_KEY\"))\n",
    "\n",
    "def create_ticket(title: str, description: str, affected_files=[], tags=[], repo_name: str=\"\"):\n",
    "    \"\"\"Create a ticket or code feature request as an output JSON file. with the given input parameters\"\"\"\n",
    "    output_folder = \"./tickets/\"\n",
    "    # Create a ticket dictionary with the provided details\n",
    "    ticket = {\n",
    "        \"title\": title,\n",
    "        \"description\": description,\n",
    "        \"affected_files\": affected_files,\n",
    "        \"tags\": tags,\n",
    "        \"status\": \"Open\"\n",
    "    }\n",
    "    \n",
    "    # Generate a unique filename based on the title\n",
    "    filename = title.replace(\" \", \"_\") + \".json\"\n",
    "    filepath = os.path.join(output_folder, filename)\n",
    "    \n",
    "    # Write the ticket to a JSON file\n",
    "    with open(filepath, 'w') as file:\n",
    "        json.dump(ticket, file, indent=4)\n",
    "\n",
    "    ticket[\"description\"] += \" Files to edit: \" + str(ticket[\"affected_files\"])\n",
    "    github_connector.create_ticket_from_json(ticket, repo_name=repo_name)\n",
    "    \n",
    "    return f\"Ticket saved to {filepath}\"\n",
    "\n",
    "docstore = Docstore(folder_path=\"./demo_app/backend/app/app/\")\n",
    "\n",
    "# tools.append(\n",
    "#     Tool(\n",
    "#         name=\"ListFiles\",\n",
    "#         func=docstore.list_files,\n",
    "#         description=\"Lists all txt files in the document store.\",\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# tools.append(\n",
    "#     Tool(\n",
    "#         name=\"ReadFile\",\n",
    "#         func=docstore.read_file,\n",
    "#         description=\"Reads the content of a specific code file.\",\n",
    "#     )\n",
    "# )\n",
    "\n",
    "ticket_tool = StructuredTool.from_function(create_ticket)\n",
    "\n",
    "\n",
    "\n",
    "tools.append(\n",
    "    ticket_tool\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frontend_repo = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "    llm=llm, chain_type=\"stuff\", retriever=frontend_docsearch.as_retriever(search_kwargs={'k': 26})\n",
    ")\n",
    "\n",
    "frontend_vector_store_tool = Tool(\n",
    "    name=\"Frontend Code repository, repo_name=zhiduozhang/frontend_sxsw_hackathon\",\n",
    "    func=frontend_repo.__call__,\n",
    "    description=\"useful for when you need to answer questions about code files within the frontend repo. Input should be a fully formed question about what code files to search. repo_name=zhiduozhang/frontend_sxsw_hackathon\",\n",
    ")\n",
    "tools.append(frontend_vector_store_tool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backend_repo = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "    llm=llm, chain_type=\"stuff\", retriever=backend_docsearch.as_retriever(search_kwargs={'k': 26})\n",
    ")\n",
    "\n",
    "backend_vector_store_tool = Tool(\n",
    "    name=\"Backend Code repository, repo_name=zhiduozhang/fastapi_demo_sxsw\",\n",
    "    func=backend_repo.__call__,\n",
    "    description=\"useful for when you need to answer questions about code files within the backend repo. Input should be a fully formed question about what code files to search. repo_name=zhiduozhang/fastapi_demo_sxsw\",\n",
    ")\n",
    "tools.append(backend_vector_store_tool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tech_lead_agent_system_prompt: str = \"\"\"\n",
    "You are a tech lead, you have access to multiple repos, given a feature request, your job is to go through the code files and create a list of tickets for a software engineer to work on\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "react = initialize_agent(tools, llm, agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_request: str = \"create issues so that there is a backend endpoint that prints ('hello world!') that gets shown in the frontend\""
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "request_prompt_template: str = \"\"\"\n",
    "For the repositories in the vector store containing all source code files, analyze the following user request:\n",
    "\n",
    "{}\n",
    "\n",
    "Your objective is to critically evaluate each repository separately to determine which files will be affected by the above request. Based on your analysis, create a set of tickets/issues for each repository that outline distinct units of work. Each ticket should:\n",
    "\n",
    "1. Specify whether it pertains to the repositories.\n",
    "2. Describe the specific work to be done.\n",
    "3. Detail the changes required.\n",
    "4. List the relevant files that will be impacted.\n",
    "\n",
    "Ensure that each ticket is clear and concise, providing enough information for a software engineer to understand and implement the necessary changes. Replace REQUEST with the actual user request or requirements string when you use this prompt.\n",
    "\"\"\"\n",
    "prompt: str = request_prompt_template.format(feature_request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "react.run(\n",
    "    prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proto",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
